{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f687d5c",
   "metadata": {},
   "source": [
    "# Spark HR Data Pipeline Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a948e16",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Install required libraries and prepare spark environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99cc1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.2/434.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting findspark\n",
      "  Using cached findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Building wheels for collected packages: pyspark, wget\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813860 sha256=aee97cb8d7b51e9964f4e003737e0a8baa8cd8c182eeeb6aaefa14cc771dee1f\n",
      "  Stored in directory: /home/maishuji/.cache/pip/wheels/00/e3/92/8594f4cee2c9fd4ad82fe85e4bf2559ab8ea84ef19b1dd3d15\n",
      "  Building wheel for wget (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9685 sha256=9fd502741f833348d28814c42407bcbe42bf6efe6a6c4a2691a6522a1cebaeed\n",
      "  Stored in directory: /home/maishuji/.cache/pip/wheels/8a/b8/04/0c88fb22489b0c049bee4e977c5689c7fe597d6c4b0e7d0b6a\n",
      "Successfully built pyspark wget\n",
      "Installing collected packages: wget, py4j, findspark, pyspark\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [pyspark]m3/4\u001b[0m [pyspark]\n",
      "\u001b[1A\u001b[2KSuccessfully installed findspark-2.0.1 py4j-0.10.9.9 pyspark-4.0.1 wget-3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.13 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installing requuired packages\n",
    "%pip install pyspark  findspark wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17809a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870e0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark is the Spark API for Python. We use PySpark to initialize the SparkContext.\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/09/10 15:32:52 WARN Utils: Your hostname, maishuji, resolves to a loopback address: 127.0.1.1; using 192.168.0.14 instead (on interface wlp4s0)\n",
      "25/09/10 15:32:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/10 15:32:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/09/10 15:32:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Creating a SparkContext object\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    \"Python Spark HR Data Pipeline Project\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f96a1",
   "metadata": {},
   "source": [
    "2. Download the CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e888896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'employees.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the CSV data first into a local `employees.csv` file\n",
    "import wget\n",
    "\n",
    "wget.download(\n",
    "    \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/data/employees.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bf5add",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Generate a Spark DataFrame from the CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a25e5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+---+----------+\n",
      "|Emp_No| Emp_Name|Salary|Age|Department|\n",
      "+------+---------+------+---+----------+\n",
      "|   198|   Donald|  2600| 29|        IT|\n",
      "|   199|  Douglas|  2600| 34|     Sales|\n",
      "|   200| Jennifer|  4400| 36| Marketing|\n",
      "|   201|  Michael| 13000| 32|        IT|\n",
      "|   202|      Pat|  6000| 39|        HR|\n",
      "|   203|    Susan|  6500| 36| Marketing|\n",
      "|   204|  Hermann| 10000| 29|   Finance|\n",
      "|   205|  Shelley| 12008| 33|   Finance|\n",
      "|   206|  William|  8300| 37|        IT|\n",
      "|   100|   Steven| 24000| 39|        IT|\n",
      "|   101|    Neena| 17000| 27|     Sales|\n",
      "|   102|      Lex| 17000| 37| Marketing|\n",
      "|   103|Alexander|  9000| 39| Marketing|\n",
      "|   104|    Bruce|  6000| 38|        IT|\n",
      "|   105|    David|  4800| 39|        IT|\n",
      "|   106|    Valli|  4800| 38|     Sales|\n",
      "|   107|    Diana|  4200| 35|     Sales|\n",
      "|   108|    Nancy| 12008| 28|     Sales|\n",
      "|   109|   Daniel|  9000| 35|        HR|\n",
      "|   110|     John|  8200| 31| Marketing|\n",
      "+------+---------+------+---+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Read data from the \"employees\" CSV file and import it into a DataFrame variable named \"employees_df\"\n",
    "employees_df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "employees_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe553fd",
   "metadata": {},
   "source": [
    "2. Define q schema for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46a4436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets first print the inferred schema\n",
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48480dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    ")\n",
    "\n",
    "# Now we define an improved schema with precise data types\n",
    "employees_schema_improved = StructType(\n",
    "    [\n",
    "    StructField(\"Emp_No\", IntegerType(), True),\n",
    "    StructField(\"Emp_Name\", StringType(), True),\n",
    "    StructField(\"Salary\", DoubleType(), True),  # Matches CSV header\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True)  # Matches CSV header\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "286f8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the new schema\n",
    "employees_df = spark.read.csv(\n",
    "    \"employees.csv\", schema=employees_schema_improved, header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "662c646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to have consistent naming conventions\n",
    "employees_df = employees_df \\\n",
    "    .withColumnRenamed(\"Emp_No\", \"employee_id\") \\\n",
    "    .withColumnRenamed(\"Emp_Name\", \"employee_name\") \\\n",
    "    .withColumnRenamed(\"Salary\", \"salary\") \\\n",
    "    .withColumnRenamed(\"Department\", \"department_name\") \\\n",
    "    .withColumnRenamed(\"Age\", \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e94753",
   "metadata": {},
   "source": [
    "3. Dislay schema of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3eef1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cca9b0",
   "metadata": {},
   "source": [
    "4. Create a temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d464d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view so that we can run SQL queries against the DataFrame\n",
    "employees_df.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794fc414",
   "metadata": {},
   "source": [
    "5. Execute an SQL query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cfebe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---+---------------+\n",
      "|employee_id|employee_name| salary|age|department_name|\n",
      "+-----------+-------------+-------+---+---------------+\n",
      "|        199|      Douglas| 2600.0| 34|          Sales|\n",
      "|        200|     Jennifer| 4400.0| 36|      Marketing|\n",
      "|        201|      Michael|13000.0| 32|             IT|\n",
      "|        202|          Pat| 6000.0| 39|             HR|\n",
      "|        203|        Susan| 6500.0| 36|      Marketing|\n",
      "|        205|      Shelley|12008.0| 33|        Finance|\n",
      "|        206|      William| 8300.0| 37|             IT|\n",
      "|        100|       Steven|24000.0| 39|             IT|\n",
      "|        102|          Lex|17000.0| 37|      Marketing|\n",
      "|        103|    Alexander| 9000.0| 39|      Marketing|\n",
      "|        104|        Bruce| 6000.0| 38|             IT|\n",
      "|        105|        David| 4800.0| 39|             IT|\n",
      "|        106|        Valli| 4800.0| 38|          Sales|\n",
      "|        107|        Diana| 4200.0| 35|          Sales|\n",
      "|        109|       Daniel| 9000.0| 35|             HR|\n",
      "|        110|         John| 8200.0| 31|      Marketing|\n",
      "|        111|       Ismael| 7700.0| 32|             IT|\n",
      "|        112|  Jose Manuel| 7800.0| 34|             HR|\n",
      "|        113|         Luis| 6900.0| 34|          Sales|\n",
      "|        116|       Shelli| 2900.0| 37|        Finance|\n",
      "+-----------+-------------+-------+---+---------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# SQL query to fetch only the records from the view where the age is greater than 30\n",
    "sql_query = \"SELECT * FROM employees WHERE age > 30\"\n",
    "result_df = spark.sql(sql_query)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef83d21",
   "metadata": {},
   "source": [
    "6. Calculate Average Salary by Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a25abf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|department_name|   average_salary|\n",
      "+---------------+-----------------+\n",
      "|             IT|           7400.0|\n",
      "|      Marketing|6633.333333333333|\n",
      "|             HR|           5837.5|\n",
      "|        Finance|           5730.8|\n",
      "|          Sales|5492.923076923077|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_query_avg_salary = \"\"\"\n",
    "SELECT department_name, AVG(salary) AS average_salary\n",
    "FROM employees\n",
    "GROUP BY department_name\n",
    "ORDER BY average_salary DESC\n",
    "\"\"\"\n",
    "avg_salary_df = spark.sql(sql_query_avg_salary)\n",
    "avg_salary_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6ab17",
   "metadata": {},
   "source": [
    "7. Filter to select records from the IT departement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58fc9d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---+---------------+\n",
      "|employee_id|employee_name| salary|age|department_name|\n",
      "+-----------+-------------+-------+---+---------------+\n",
      "|        198|       Donald| 2600.0| 29|             IT|\n",
      "|        201|      Michael|13000.0| 32|             IT|\n",
      "|        206|      William| 8300.0| 37|             IT|\n",
      "|        100|       Steven|24000.0| 39|             IT|\n",
      "|        104|        Bruce| 6000.0| 38|             IT|\n",
      "|        105|        David| 4800.0| 39|             IT|\n",
      "|        111|       Ismael| 7700.0| 32|             IT|\n",
      "|        129|        Laura| 3300.0| 38|             IT|\n",
      "|        132|           TJ| 2100.0| 34|             IT|\n",
      "|        136|        Hazel| 2200.0| 29|             IT|\n",
      "+-----------+-------------+-------+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.filter(employees_df.department_name == \"IT\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450802b",
   "metadata": {},
   "source": [
    "8. Add 10% Bonus to Salaries ( with new column `salary_after_bonus`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd6090f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---+---------------+------------------+\n",
      "|employee_id|employee_name| salary|age|department_name|salary_after_bonus|\n",
      "+-----------+-------------+-------+---+---------------+------------------+\n",
      "|        198|       Donald| 2600.0| 29|             IT|2860.0000000000005|\n",
      "|        199|      Douglas| 2600.0| 34|          Sales|2860.0000000000005|\n",
      "|        200|     Jennifer| 4400.0| 36|      Marketing|            4840.0|\n",
      "|        201|      Michael|13000.0| 32|             IT|14300.000000000002|\n",
      "|        202|          Pat| 6000.0| 39|             HR| 6600.000000000001|\n",
      "|        203|        Susan| 6500.0| 36|      Marketing| 7150.000000000001|\n",
      "|        204|      Hermann|10000.0| 29|        Finance|           11000.0|\n",
      "|        205|      Shelley|12008.0| 33|        Finance|13208.800000000001|\n",
      "|        206|      William| 8300.0| 37|             IT|            9130.0|\n",
      "|        100|       Steven|24000.0| 39|             IT|26400.000000000004|\n",
      "|        101|        Neena|17000.0| 27|          Sales|           18700.0|\n",
      "|        102|          Lex|17000.0| 37|      Marketing|           18700.0|\n",
      "|        103|    Alexander| 9000.0| 39|      Marketing|            9900.0|\n",
      "|        104|        Bruce| 6000.0| 38|             IT| 6600.000000000001|\n",
      "|        105|        David| 4800.0| 39|             IT|            5280.0|\n",
      "|        106|        Valli| 4800.0| 38|          Sales|            5280.0|\n",
      "|        107|        Diana| 4200.0| 35|          Sales|            4620.0|\n",
      "|        108|        Nancy|12008.0| 28|          Sales|13208.800000000001|\n",
      "|        109|       Daniel| 9000.0| 35|             HR|            9900.0|\n",
      "|        110|         John| 8200.0| 31|      Marketing|            9020.0|\n",
      "+-----------+-------------+-------+---+---------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "employees_df = employees_df.withColumn(\"salary_after_bonus\", col(\"salary\") * 1.1)\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f07ee5",
   "metadata": {},
   "source": [
    "9. Find Maximum Salary by Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "277e80a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|max(salary)|\n",
      "+---+-----------+\n",
      "| 31|     8200.0|\n",
      "| 34|     7800.0|\n",
      "| 28|    12008.0|\n",
      "| 27|    17000.0|\n",
      "| 26|     3600.0|\n",
      "| 37|    17000.0|\n",
      "| 35|     9000.0|\n",
      "| 39|    24000.0|\n",
      "| 38|     6000.0|\n",
      "| 29|    10000.0|\n",
      "| 32|    13000.0|\n",
      "| 33|    12008.0|\n",
      "| 30|     8000.0|\n",
      "| 36|     7900.0|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "employees_df.groupBy(\"age\").agg(max(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659f4d5",
   "metadata": {},
   "source": [
    "10. Self-Join on Employee Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b1d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-10 16:36:50.838\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employees`.`employee_id` cannot be resolved. Did you mean one of the following? [`employee_id`, `employee_id`, `employee_name`, `employee_name`, `salary_after_bonus`]. SQLSTATE: 42703\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o145.join.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employees`.`employee_id` cannot be resolved. Did you mean one of the following? [`employee_id`, `employee_id`, `employee_name`, `employee_name`, `salary_after_bonus`]. SQLSTATE: 42703;\\n'Join Inner, '`=`('employees.employee_id, 'employees.age)\\n:- Project [employee_id#140, employee_name#141, salary#142, age#144, department_name#143, (salary#142 * 1.1) AS salary_after_bonus#231]\\n:  +- Project [employee_id#140, employee_name#141, salary#142, Age#137 AS age#144, department_name#143]\\n:     +- Project [employee_id#140, employee_name#141, salary#142, Age#137, Department#138 AS department_name#143]\\n:        +- Project [employee_id#140, employee_name#141, Salary#136 AS salary#142, Age#137, Department#138]\\n:           +- Project [employee_id#140, Emp_Name#135 AS employee_name#141, Salary#136, Age#137, Department#138]\\n:              +- Project [Emp_No#134 AS employee_id#140, Emp_Name#135, Salary#136, Age#137, Department#138]\\n:                 +- Relation [Emp_No#134,Emp_Name#135,Salary#136,Age#137,Department#138] csv\\n+- Project [employee_id#294, employee_name#295, salary#296, age#298, department_name#297, (salary#296 * 1.1) AS salary_after_bonus#299]\\n   +- Project [employee_id#294, employee_name#295, salary#296, Age#292 AS age#298, department_name#297]\\n      +- Project [employee_id#294, employee_name#295, salary#296, Age#292, Department#293 AS department_name#297]\\n         +- Project [employee_id#294, employee_name#295, Salary#291 AS salary#296, Age#292, Department#293]\\n            +- Project [employee_id#294, Emp_Name#290 AS employee_name#295, Salary#291, Age#292, Department#293]\\n               +- Project [Emp_No#289 AS employee_id#294, Emp_Name#290, Salary#291, Age#292, Department#293]\\n                  +- Relation [Emp_No#289,Emp_Name#290,Salary#291,Age#292,Department#293] csv\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.Dataset.withPlan(Dataset.scala:2263)\\n\\tat org.apache.spark.sql.classic.Dataset.resolveSelfJoinCondition(Dataset.scala:666)\\n\\tat org.apache.spark.sql.classic.Dataset.join(Dataset.scala:691)\\n\\tat org.apache.spark.sql.classic.Dataset.join(Dataset.scala:232)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:251)\\n\\t\\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\\n\\t\\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\\n\\t\\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:251)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 22 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/home/maishuji/.local/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/home/maishuji/.local/lib/python3.13/site-packages/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employees`.`employee_id` cannot be resolved. Did you mean one of the following? [`employee_id`, `employee_id`, `employee_name`, `employee_name`, `salary_after_bonus`]. SQLSTATE: 42703;\n'Join Inner, '`=`('employees.employee_id, 'employees.age)\n:- Project [employee_id#140, employee_name#141, salary#142, age#144, department_name#143, (salary#142 * 1.1) AS salary_after_bonus#231]\n:  +- Project [employee_id#140, employee_name#141, salary#142, Age#137 AS age#144, department_name#143]\n:     +- Project [employee_id#140, employee_name#141, salary#142, Age#137, Department#138 AS department_name#143]\n:        +- Project [employee_id#140, employee_name#141, Salary#136 AS salary#142, Age#137, Department#138]\n:           +- Project [employee_id#140, Emp_Name#135 AS employee_name#141, Salary#136, Age#137, Department#138]\n:              +- Project [Emp_No#134 AS employee_id#140, Emp_Name#135, Salary#136, Age#137, Department#138]\n:                 +- Relation [Emp_No#134,Emp_Name#135,Salary#136,Age#137,Department#138] csv\n+- Project [employee_id#294, employee_name#295, salary#296, age#298, department_name#297, (salary#296 * 1.1) AS salary_after_bonus#299]\n   +- Project [employee_id#294, employee_name#295, salary#296, Age#292 AS age#298, department_name#297]\n      +- Project [employee_id#294, employee_name#295, salary#296, Age#292, Department#293 AS department_name#297]\n         +- Project [employee_id#294, employee_name#295, Salary#291 AS salary#296, Age#292, Department#293]\n            +- Project [employee_id#294, Emp_Name#290 AS employee_name#295, Salary#291, Age#292, Department#293]\n               +- Project [Emp_No#289 AS employee_id#294, Emp_Name#290, Salary#291, Age#292, Department#293]\n                  +- Relation [Emp_No#289,Emp_Name#290,Salary#291,Age#292,Department#293] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m joined_df = \u001b[43memployees_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43memployees_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memployees.employee_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memployees.age\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyspark/sql/classic/dataframe.py:710\u001b[39m, in \u001b[36mDataFrame.join\u001b[39m\u001b[34m(self, other, on, how)\u001b[39m\n\u001b[32m    708\u001b[39m         on = \u001b[38;5;28mself\u001b[39m._jseq([])\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(how, \u001b[38;5;28mstr\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mhow should be a string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `employees`.`employee_id` cannot be resolved. Did you mean one of the following? [`employee_id`, `employee_id`, `employee_name`, `employee_name`, `salary_after_bonus`]. SQLSTATE: 42703;\n'Join Inner, '`=`('employees.employee_id, 'employees.age)\n:- Project [employee_id#140, employee_name#141, salary#142, age#144, department_name#143, (salary#142 * 1.1) AS salary_after_bonus#231]\n:  +- Project [employee_id#140, employee_name#141, salary#142, Age#137 AS age#144, department_name#143]\n:     +- Project [employee_id#140, employee_name#141, salary#142, Age#137, Department#138 AS department_name#143]\n:        +- Project [employee_id#140, employee_name#141, Salary#136 AS salary#142, Age#137, Department#138]\n:           +- Project [employee_id#140, Emp_Name#135 AS employee_name#141, Salary#136, Age#137, Department#138]\n:              +- Project [Emp_No#134 AS employee_id#140, Emp_Name#135, Salary#136, Age#137, Department#138]\n:                 +- Relation [Emp_No#134,Emp_Name#135,Salary#136,Age#137,Department#138] csv\n+- Project [employee_id#294, employee_name#295, salary#296, age#298, department_name#297, (salary#296 * 1.1) AS salary_after_bonus#299]\n   +- Project [employee_id#294, employee_name#295, salary#296, Age#292 AS age#298, department_name#297]\n      +- Project [employee_id#294, employee_name#295, salary#296, Age#292, Department#293 AS department_name#297]\n         +- Project [employee_id#294, employee_name#295, Salary#291 AS salary#296, Age#292, Department#293]\n            +- Project [employee_id#294, Emp_Name#290 AS employee_name#295, Salary#291, Age#292, Department#293]\n               +- Project [Emp_No#289 AS employee_id#294, Emp_Name#290, Salary#291, Age#292, Department#293]\n                  +- Relation [Emp_No#289,Emp_Name#290,Salary#291,Age#292,Department#293] csv\n"
     ]
    }
   ],
   "source": [
    "joined_df = employees_df.join(employees_df, col(\"employees.employee_id\") == col(\"employees.employee_id\"), \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889043f0",
   "metadata": {},
   "source": [
    "11. Calculate Average Employee Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d090828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|average_age|\n",
      "+-----------+\n",
      "|      33.56|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg \n",
    "employees_df.agg(avg(col(\"age\")).alias(\"average_age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f4e454",
   "metadata": {},
   "source": [
    "12. Calculate Total Salary By Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "527dcd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|department_name|sum(salary)|\n",
      "+---------------+-----------+\n",
      "|          Sales|    71408.0|\n",
      "|             HR|    46700.0|\n",
      "|        Finance|    57308.0|\n",
      "|      Marketing|    59700.0|\n",
      "|             IT|    74000.0|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum \n",
    "\n",
    "salary_per_department_df = employees_df.groupBy(\"department_name\").agg(sum(\"salary\"))\n",
    "salary_per_department_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237bd0f",
   "metadata": {},
   "source": [
    "13. Sort Data by Age and Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bde6f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+---+---------------+------------------+\n",
      "|employee_id|employee_name| salary|age|department_name|salary_after_bonus|\n",
      "+-----------+-------------+-------+---+---------------+------------------+\n",
      "|        137|       Renske| 3600.0| 26|      Marketing|3960.0000000000005|\n",
      "|        101|        Neena|17000.0| 27|          Sales|           18700.0|\n",
      "|        114|          Den|11000.0| 27|        Finance|12100.000000000002|\n",
      "|        108|        Nancy|12008.0| 28|          Sales|13208.800000000001|\n",
      "|        130|        Mozhe| 2800.0| 28|      Marketing|3080.0000000000005|\n",
      "|        126|        Irene| 2700.0| 28|             HR|2970.0000000000005|\n",
      "|        204|      Hermann|10000.0| 29|        Finance|           11000.0|\n",
      "|        115|    Alexander| 3100.0| 29|        Finance|3410.0000000000005|\n",
      "|        134|      Michael| 2900.0| 29|          Sales|3190.0000000000005|\n",
      "|        198|       Donald| 2600.0| 29|             IT|2860.0000000000005|\n",
      "|        140|       Joshua| 2500.0| 29|        Finance|            2750.0|\n",
      "|        136|        Hazel| 2200.0| 29|             IT|            2420.0|\n",
      "|        120|      Matthew| 8000.0| 30|             HR|            8800.0|\n",
      "|        110|         John| 8200.0| 31|      Marketing|            9020.0|\n",
      "|        127|        James| 2400.0| 31|             HR|            2640.0|\n",
      "|        201|      Michael|13000.0| 32|             IT|14300.000000000002|\n",
      "|        111|       Ismael| 7700.0| 32|             IT|            8470.0|\n",
      "|        119|        Karen| 2500.0| 32|        Finance|            2750.0|\n",
      "|        205|      Shelley|12008.0| 33|        Finance|13208.800000000001|\n",
      "|        124|        Kevin| 5800.0| 33|      Marketing| 6380.000000000001|\n",
      "+-----------+-------------+-------+---+---------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "employees_df.sort(col(\"age\").asc(), col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1af8f",
   "metadata": {},
   "source": [
    "14. Count Employees in Each Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f129527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|department_name|employee_count|\n",
      "+---------------+--------------+\n",
      "|          Sales|            13|\n",
      "|             HR|             8|\n",
      "|        Finance|            10|\n",
      "|      Marketing|             9|\n",
      "|             IT|            10|\n",
      "+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "employees_df.groupBy(\"department_name\").agg(count(\"employee_id\").alias(\"employee_count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5613ae39",
   "metadata": {},
   "source": [
    "15. Filter Employees with the letter o in the Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc9e4ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+---+---------------+------------------+\n",
      "|employee_id|employee_name|salary|age|department_name|salary_after_bonus|\n",
      "+-----------+-------------+------+---+---------------+------------------+\n",
      "|        198|       Donald|2600.0| 29|             IT|2860.0000000000005|\n",
      "|        199|      Douglas|2600.0| 34|          Sales|2860.0000000000005|\n",
      "|        110|         John|8200.0| 31|      Marketing|            9020.0|\n",
      "|        112|  Jose Manuel|7800.0| 34|             HR|            8580.0|\n",
      "|        130|        Mozhe|2800.0| 28|      Marketing|3080.0000000000005|\n",
      "|        133|        Jason|3300.0| 38|          Sales|3630.0000000000005|\n",
      "|        139|         John|2700.0| 36|          Sales|2970.0000000000005|\n",
      "|        140|       Joshua|2500.0| 29|        Finance|            2750.0|\n",
      "+-----------+-------------+------+---+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees_df.filter(col(\"employee_name\").contains(\"o\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
